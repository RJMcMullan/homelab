{"config":{"lang":["en"],"separator":"[\\s\\-\\.]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#current-network-setup","title":"Current Network setup","text":"<p>So my current setup is this. The internet comes in the my home using my local ISP ONT box which gives me full 1 Gb fiber, excellent! The nokia ISP router is put into bridged mode so that I can have the main opnsense router (in green) do all the routing for me. I have VLANS, DHCP and other services running on my opnsense which i would rather use than the bog standard Nokia router. I'll go through this setup later.</p> <p>The opnsense router LAN port is connected to an 8 port TP-Link switch, which has VLANS configured on it and this switch is connected to an 16 port TP-Link poe switch up stair in my office. To pass the VLAN information the ports on both switch have to be tagged with all the VLANS so that traffic can be routed to the correct VLANS. </p> <p></p>"},{"location":"#vlan-configuration-8-port-switch","title":"VLAN configuration (8 port switch)","text":"<p>TP-link TL-SG1016PE </p>"},{"location":"#vlan-configuration-16-port-switch","title":"VLAN configuration (16 port switch)","text":"<p>TP-link TL-SG108E </p>"},{"location":"blog/","title":"My Blog","text":"<p>Hello! Disclaimer: I\u2019m no prodigy in the realms of what I scribble here, learning is a journey and I don\u2019t think you ever stop off on that journey! Over the past few years my home lab has progressed quite drastically, from a basic ISP network with a single router and a home servers / containers running pihole to a full home lab network which is running, opnsense, a proxmox ve cluster running docker containers and other services and a k8s cluster deployed using kubespray. It has been developed from what I have learned working as a DevOps Engineer, stuff I\u2019ve figured out along the way during the setup as well as stuff I have read from other people\u2019s helpful blog posts.</p> <p>The aim of this blog is to outline my current setup, to not only help myself remember how everything is configured, but also it might help someone else along their home lab setup journey too. Let\u2019s dive in and see what quirky adventures we can uncover together! So here goes, happy home labbing!</p>"},{"location":"blog/#about-me","title":"About me","text":"<p>As an energetic DevOps Engineer, I bring extensive technical proficiency and hands-on involvement across various systems and technologies in my current position\u2014essentially, it's like having to be Swiss Army knife, but with more YAML and fewer sharp edges. My disciplined approach and self-reliance allow me to excel with minimal oversight, while my collaborative nature fosters seamless teamwork (and I even occasionally tolerate meetings that could\u2019ve been emails).</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/","title":"HAProxy + Traefik + Let's Encrypt Wildcard Certificates + 100% A+ ssl Rating","text":""},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#configuration-guide","title":"Configuration Guide","text":"<p>This is the help anyone wanting to implement a reverse proxy into their homelab environment, for hosting personal websites and projects to the public.</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-1-plugin-installation","title":"Part 1 - Plugin Installation","text":"<p>1. Update your OPNsense to the latest version and ensure you're using the OpenSSL firmware flavor, as LibreSSL doesn't support TLS 1.3. - Navigate to \"System \u2192 Firmware \u2192 Updates\" and install all updates.</p> <p>2. Install necessary plugins: - Go to \"System \u2192 Firmware \u2192 Plugins\" and install the following: <code>os-acme-client</code>, <code>os-ddclient</code>, <code>os-haproxy</code>.</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-2-dynamic-dns-configuration-using-cloudflare","title":"Part 2 - Dynamic DNS Configuration using cloudflare","text":"<p>1. Create and verify an account on cloudflare.</p> <p>2. Create your subdomain: - Go to cloudflare &gt; websites &gt; domain name &gt; DNS &gt; records and create your domain (e.g. <code>homelab.devopsjourneys.wiki</code>).</p> <p></p> <p>3. Generate and save a token:    - Log in to your Cloudflare account and navigate to the dashboard.    - Click on the profile icon in the top right corner and select \"My Profile.\"    - In the left-hand menu, click on \"API Tokens,\" then select \"Create Token.\"    - Choose the \"Edit zone DNS\" template from the list of API token templates.    - In the \"Zone Resources\" section, select the desired domain from the right-most dropdown menu, then click \"Continue to summary.\"    - Verify that the API token has DNS    - permission for the selected domain and click \"Create Token.\"</p> <p>4. Setup the Dynamic DNS plugin in OPNSENSE and enter the following information:</p> <ul> <li>Enabled: true</li> <li>Description: Cloudflare (Or whatever you want)</li> <li>Service: Cloudflare</li> <li>Username: token (the word, not your API token)</li> <li>Password:  <li>Zone: your.domain (e.g. devopsjourneys.wiki)</li> <li>Hostname: full domain name (e.g. homelab.devopsjourneys.wiki)</li> <li>Check ip method: Interface</li> <li>Interface to monitor: WAN</li> <li>Force SSL: true    </li>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-3-lets-encrypt-acme-client","title":"Part 3 - Let's Encrypt (ACME Client)","text":"<p>1. Configure ACME Client settings in OPNsense:</p> <p>- Navigate to \"Services \u2192 ACME Client \u2192 Settings\" and adjust settings as this image.</p> <p>2. Set update schedule for certificate renewal:</p> <p>In this section, we will set the time of day for certificate renewals. Certificates won't be renewed daily; the ACME client first checks if they are nearing expiration and only renews them if necessary. Schedule the renewal for a time when your services experience low traffic, as the ACME plugin will restart HAProxy to apply the new certificates, causing a brief downtime. Avoid scheduling renewals at exact hours (e.g., 3:00 AM) since Let's Encrypt servers might be overloaded, potentially causing the renewal process to fail   - Go to \"Services \u2192 ACME Client \u2192 Settings \u2192 Update Schedule\" and configure a time with low service load. 3. Create an ACME account:   - Go to \"Services \u2192 ACME Client \u2192 Accounts\" and create an account using the staging environment first. </p> <p>4. Set up automation:   - Create an automation to restart HAProxy after certificate renewal.  5. Add DNS challenge:   - Go to \"Services \u2192 ACME Client \u2192 Challenge Types\" and add the DNS challenge for deSEC.  6. Add your certificate:   - Configure the certificate in \"Services \u2192 ACME Client \u2192 Certificates\".   I prefer using Elliptic Curve Cryptography (ECC) (https://en.wikipedia.org/wiki/Elliptic-curve_cryptography). However, you can also use RSA keys. If you choose RSA, ensure the key length is as long as possible to achieve an A+ rating from SSLLabs.   - Forcefully issue a staging certificate for testing, then switch to the production environment and issue the certificate by selecting the highlighted part here: </p> <p>### Part 4 - System Preparation</p> <p>1. Adjust system settings:   - Go to \"System \u2192 Settings \u2192 Administration\" and change the Web GUI TCP port (anything other than 443), and disable the web GUI redirect rule.</p> <p>2. Optional: Configure Virtual IPs: Create a virtual IP in a subnet different from any of your other networks. Ideally, select an IP from the localhost subnet to avoid conflicts in your local network (https://en.wikipedia.org/wiki/Localhost). In this tutorial, we will use \"127.4.4.3/32\" as the IP address for both the \"HTTP_frontend\" and \"HTTPS_frontend\".   - Create a virtual IP in \"Interfaces \u2192 Virtual IPs \u2192 Settings\" if not using localhost.  3. Create a firewall alias:   - Go to \"Firewall \u2192 Aliases\" and create an alias for HAProxy ports (port 80 and 443).  4. Set up firewall rules:   - Allow inbound traffic on HAProxy ports via \"Firewall \u2192 Rules \u2192 WAN\". </p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-5-haproxy-configuration","title":"Part 5 - HAProxy Configuration","text":"<p>1. Configure HAProxy settings:   - Go to \"Services \u2192 HAProxy \u2192 Settings \u2192 Service\" and adjust as needed.  \u00a0 \u00a0- Update \"Global Parameters\" and \"Default Parameters\" in HAProxy settings.  </p> <p>2. Add real servers and backend pools:   - Here we add the \"Real Servers\" the internal IP in my case of my traefik instance running on docker, the description and port   - The wildcard certificate we created above is also added here *.devopsjourney.wiki    </p> <ul> <li>Set up backend pools for services in \"Virtual Services \u2192 Backend Pools\".</li> </ul> <p>  3. Create rules and conditions:   - Add a NoSSL_condition\", which is necessary in order to identify non-HTTPS traffic      - Create a HTTPtoHTTPS rule. This is so the client connection gets upgraded from HTTP to HTTPS and connects to the HTTPS_frontend   - Create map files for public subdomains.  Here, we will generate a new map file named \"PUBLIC_SUBDOMAINS_mapfile\" for the public subdomains we want to access externally from our network. HAProxy map files are used to store key-value pairs that can be referenced by HAProxy during its operation. These map files allow for efficient lookups and are typically used for tasks such as URL rewriting, header modification, or directing traffic based on specific criteria. Key features of HAProxy map files include: - Format: Each line in a map file represents a key-value pair, separated by spaces or tabs. The key is usually some form of input (like a URL or host) and the value is the corresponding output (like a backend server or a rewritten URL).</p> <p>Example:   subdomain1.example.com backend1   subdomain2.example.com backend2</p> <ul> <li>Usage: Map files are referenced in HAProxy configuration using directives such as use-server, acl, or http-request. This enables dynamic behavior based on the contents of the map file.</li> <li>Efficiency: Map files allow HAProxy to handle complex routing and transformation rules efficiently without hardcoding them into the configuration, enabling easier updates and maintenance.</li> <li> <p>Reloading: HAProxy can reload map files on the fly without requiring a full restart, which minimizes downtime and allows for dynamic updates to routing logic.</p> </li> <li> <p>Now create the Map file and PUBLIC_SUBDOMAINS_map-rule as follows    </p> </li> </ul> <p>4. Configure Public Services (frontends):   - Set up SNI, HTTP, and HTTPS frontends in \"Virtual Services \u2192 Public Services\".   Here, we will set up the frontends to listen on our interface IPs and the virtual IP we created earlier. First, we will create the \"SNI_frontend,\" which will determine whether the traffic should be SSL offloaded. You will need to place the rules for all services that you don't want to be SSL offloaded here. The default backend for this frontend will be the \"SSL_backend,\" which redirects all traffic to our virtual \"SSL_server,\" corresponding to our \"HTTPS_frontend.\"      - Now we will set up our \"HTTP_frontend.\" Ensure that the \"HTTPtoHTTPS_rule\" is included in this frontend! This frontend is essential for redirecting HTTP traffic to HTTPS, but it can also be used to serve non-SSL encrypted services on port 80.       - Now we will set up our \"HTTPS_frontend.\" This will be our primary frontend, performing SSL offloading using the Let's Encrypt certificate we created earlier. You should include the \"PUBLIC_SUBDOMAINS_rule\" and all other rules for services that need SSL offloading here. The \"Cipher List\" and \"Cipher Suites\" will ensure we achieve a 100% A+ rating at SSLLabs.</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-6-internal-network-access","title":"Part 6 - Internal Network Access","text":"<p>1. Option A: Split DNS:    If you attempt to access your URL \"your_service.your_subdomain.dedyn.io\" from a device within your internal network, it should fail There are two ways to resolve this issue. I will cover both options, but note that Split DNS (Option A) is the recommended approach. NAT Reflection (Option B) is a less favorable solution because it causes you to lose the ability to track the originating source IP in HAProxy when using NAT.</p> <p>Option A - Split DNS (https://docs.opnsense.org/manual/unbound.html#overrides) Option B - NAT Reflection (https://docs.opnsense.org/manual/nat.html)   - Use Unbound DNS plugin for DNS overrides.   - Go to \"Services \u2192 Unbound DNS \u2192 Overrides\" and create host overrides for each of your services. This applies if you are using second-level subdomains like \"your_service.your_subdomain.dedyn.io\" for your services. If all your services run on your first-level subdomain \"your_subdomain.dedyn.io,\" you only need to override this one. The IP address can be any LAN (or VLAN) interface IP of your OPNsense. I am using the LAN IP that the \"SNI_frontend\" is also listening on, since we set it to \"0.0.0.0\".</p> <p>2. Option B: NAT Reflection:</p> <ul> <li>Edit port forwarding rules in \"Firewall \u2192 NAT \u2192 Port Forward\" to enable NAT reflection.</li> </ul>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-7-local-access-only-subdomains","title":"Part 7 - Local-Access-Only Subdomains","text":"<p>1. Create a local map file:  - In OPNsense, navigate to: Services \u2192 HAProxy \u2192 Settings \u2192 Advanced \u2192 Map Files. Here, clone the \"PUBLIC_SUBDOMAINS_mapfile,\" rename it to something like \"LOCAL_SUBDOMAINS_mapfile,\" and add all your local-access-only subdomains along with their corresponding backends. Remember that the contents of your \"PUBLIC_SUBDOMAINS_mapfile\" must also be included in the \"LOCAL_SUBDOMAINS_mapfile.\" I will explain the reason for this later.</p> <p>2. Add local conditions and rules:  - Next, go to: Services \u2192 HAProxy \u2192 Settings \u2192 Rules &amp; Checks \u2192 Conditions. Create a condition to detect if the source of the request is a local IP or a FQDN. You can use the predefined \"Source IP is local\" condition, but I prefer to use specific subnets since the predefined condition covers the entire RFC1918 IP range, which is more than I need. As mentioned, you can also check for a FQDN. However, keep in mind that HAProxy resolves these hostnames to their IPs at startup or restart. If the IP of your FQDN changes frequently, this won't work well unless you restart HAProxy regularly, such as every 24 hours using a cron job.  - Next, go to: Services \u2192 HAProxy \u2192 Settings \u2192 Rules &amp; Checks \u2192 Rules. Clone the \"PUBLIC_SUBDOMAINS_rule,\" rename it to something like \"LOCAL_SUBDOMAINS_rule,\" select your \"LOCAL_SUBDOMAINS_SUBNETS_condition,\" and choose your \"LOCAL_SUBDOMAINS_mapfile.\" If you are also using a FQDN condition, like I am, you need to select both your FQDN and subnet conditions with the logical \"or\" operator.    - Finally, go to: Services \u2192 HAProxy \u2192 Settings \u2192 Virtual Services \u2192 Public Services. Place the \"LOCAL_SUBDOMAINS_rule\" before your \"PUBLIC_SUBDOMAINS_rule\" in your \"HTTPS_frontend.\" </p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-8-hide-certificate-on-ip-access","title":"Part 8 - Hide Certificate on IP Access","text":"<p>1. Enable strict SNI:   - Edit \"HTTPS_frontend\" in \"Virtual Services \u2192 Public Services\".   - Change \"curves secp384r1\" to \"curves secp384r1 strict-sni\" in the SSL options.</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-9-ssllabs-testing","title":"Part 9 - ssllabS testing","text":"<p>We are now going to test our configuration for secureness. 1. Go to https://www.ssllabs.com/ssltest/ 2. Enter your domain to test 3. This will perform deep ssl testing on your domain 4. Wait for the results. You should now have an A+ rating </p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#part-10-traefik-setup","title":"Part 10 - Traefik setup","text":"<p>Here we are going to use our HAProxy as a reverse proxy in front of Traefik, which then proxies requests to this blog, you'll need to carefully configure each component to work together</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#prerequisites","title":"Prerequisites","text":"<p>1. Docker: Ensure Docker is installed and running on your server. 2. Docker Compose: Ensure Docker Compose is installed. 3. HAProxy: Familiarity with HAProxy configuration.</p>"},{"location":"cloudflared/traefik/cloudflared-ha-proxy-traefik/#step-by-step-guide","title":"Step-by-Step Guide","text":"<p>1. Create a Docker Network for traefik and database network to be used by the blog Create a Docker network that all services will use to communicate.</p> <p><pre><code>  docker network create traefik\n  docker network create db\n</code></pre> 2. Deploy traefik using docker compose</p> <pre><code>networks:\n  traefik:\n    external: true\nvolumes:\n  traefik_config:\n    external: true\n  lets_encrypt:\n  traefik_dynamic_config:\n    external: true\nservices:\n  reverse-proxy:\n    # The official v2 Traefik docker image\n    image: traefik:v2.11.2\n    # Enables the web UI and tells Traefik to listen to docker\n    command: --api.insecure=true --metrics.prometheus=true --metrics.prometheus.manualrouting=true\n      - \"--entrypoints.websecure.address=:443\"\n      - \"--entrypoints.web.address=:80\"\n      - \"--log.level=DEBUG\"\n      - \"--configFile=/etc/traefik/traefik.yml\"\n    ports:\n      # The HTTP port\n      - \"80:80\"\n      - \"443:443\"\n      # The Web UI (enabled by --api.insecure=true)\n      - \"8080:8080\"\n      - \"8082:8082\"\n    expose:\n      - 8082\n    restart: unless-stopped\n    volumes:\n      # So that Traefik can listen to the Docker events\n      - /var/run/docker.sock:/var/run/docker.sock\n      - \"traefik_config:/etc/traefik/\"\n      # Mount the dynamic configuration\n      # Mount the cert directory\n      - \"./certs/:/etc/certs/\"\n    networks:\n      - traefik\n    logging:\n      driver: fluentd\n      options:\n        fluentd-address: 127.0.0.1:24224\n        fluentd-async: 'true'\n        tag: docker.traefik\n\n    labels:\n      # Dynamic configuration with Docker Labels\n      # Ref: https://docs.traefik.io/reference/dynamic-configuration/docker/\n      # Explicitly tell Traefik to expose this container\n      - traefik.enable=true\n      # Allow request only from the predefined entry point named \"web\"\n      - traefik.http.routers.traefik-ui.entrypoints=web\n      # The port the dashboard responds on\n      - traefik.http.services.traefik-ui.loadbalancer.server.port=8080\n      # The URL for the traefik ui\n      - traefik.http.routers.traefik-ui.rule=Host(`traefik-docker01.internal`)\n</code></pre> <p>3. Deploy wiki.js</p> <p><pre><code> services:\n\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: wiki\n      POSTGRES_PASSWORD: wikijsrocks\n      POSTGRES_USER: wikijs\n    networks:\n      - db\n    logging:\n      driver: \"fluentd\"\n      options:\n        fluentd-address: 127.0.0.1:24224\n        fluentd-async: 'true'\n        tag: docker.postgres_wiki\n    labels:\n      - \"com.centurylinklabs.watchtower.enable=false\"\n    restart: unless-stopped\n    volumes:\n      - db-data:/var/lib/postgresql/data\n\n  wiki:\n    image: ghcr.io/requarks/wiki:2\n    depends_on:\n      - db\n    networks:\n      - traefik\n      - db\n    ports:\n      - \"8084:3000\"\n    environment:\n      ADMIN_EMAIL:\n      ADMIN_PASS: wikiadmin\n      DB_TYPE: postgres\n      DB_PORT: 5432\n      DB_HOST: db\n      DB_USER: wikijs\n      DB_PASS: wikijsrocks\n      DB_NAME: wiki\n    restart: unless-stopped\n    logging:\n      driver: \"fluentd\"\n      options:\n        fluentd-address: 127.0.0.1:24224\n        fluentd-async: 'true'\n        tag: docker.wiki\n\n    labels:\n      # Dynamic configuration with Docker Labels\n      # Ref: https://docs.traefik.io/reference/dynamic-configuration/docker/\n      # Explicitly tell Traefik to expose this container\n      - traefik.enable=true\n      # The domain the service will respond to\n      #- traefik.http.routers.wiki-web.rule=Host(`wiki-docker01.internal`)\n      - traefik.http.routers.wiki-web.rule=Host(`homelab.devopsjourney.wiki`)\n      # Allow request only from the predefined entry point named \"web\"\n      - traefik.http.routers.wiki-web.entrypoints=web\n      # if you have multiple ports exposed on the service, specify port in the web-secure service\n      - traefik.http.services.wiki-web.loadbalancer.server.port=3000\n      - \"com.centurylinklabs.watchtower.enable=false\"\n\n\nvolumes:\n  db-data:\nnetworks:\n  traefik:\n    external: true\n  db:\n    external: true\n</code></pre> Things to note:   - Make sure you set the traefik.http.routers.wiki-web.rule=Host to match your public DNS record in cloudflare</p> <p>4. Go through steps 5 - 9 above to add the service to HAProxy in OPNsense  </p>"},{"location":"cloudflared/traefik/cloudflared-traefik/","title":"cloudflared + traefik + docker for web-hosting without opened ports","text":""},{"location":"cloudflared/traefik/cloudflared-traefik/#introduction","title":"Introduction","text":"<p>This setup could potentially address scenarios such as serving web content from behind a carier grade NAT (CNAT) or in situations where a public IP isn't available. Surprisingly, the solution requires just one additional component: Cloudflare Tunnels. These tunnels establish a link between my server and Cloudflare without necessitating opened ports. In this blog post, I'll delve into my setup for achieving full encryption between the web service and my users, along with key considerations for similar implementations.</p> <p>To read more about cloudflared tunnels view here: https://developers.cloudflare.com/cloudflare-one/connections/connect-networks/</p>"},{"location":"cloudflared/traefik/cloudflared-traefik/#configuration","title":"Configuration","text":""},{"location":"cloudflared/traefik/cloudflared-traefik/#traefik-cloudflared","title":"Traefik + cloudflared","text":"<p>Cloudflare Tunnels are operated via cloudflared, a software daemon provided by Cloudflare that initiates an outbound connection to the nearest Cloudflare points of presence to proxy internet traffic. As Cloudflare Tunnels offer limited routing functionality, specifically path-based routing, we combine them with the widely used reverse proxy Traefik, which integrates seamlessly with Docker. Cloudflare directs traffic to Traefik, which in turn routes it to individual Docker containers housing the web services we aim to expose.</p> <p>Below is the docker-compose.yml configuration for Traefik and cloudflared.</p> <p><pre><code>networks:\n  cftunnel-transport:\n    external: true\n  cloudflaretunnel:\n    external: true\n\nservices:\n  tunnel:\n    container_name: cloudflared-tunnel-traefik\n    image: cloudflare/cloudflared:2023.10.0-amd64\n    restart: unless-stopped\n    command: tunnel run\n    environment:\n      - \"TUNNEL_TOKEN=\"\n    networks:\n      - cftunnel-transport\n\n  traefik:\n    image: traefik:v2.10.7\n    container_name: traefik_cloudflare_ingress\n    restart: always\n    networks:\n      - cftunnel-transport\n      - cloudflaretunnel\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - ./traefik.yml:/traefik.yml:ro\n      - ./certificates.yml:/certificates.yml:ro\n      - ./origin-certificates/:/origin-certificates:ro\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.traefik-ui.rule=Host(`traefik-cf-docker01.internal`)\"\n      - \"traefik.http.services.traefik-ui.loadbalancer.server.port=8085\"\n    logging:\n      driver: fluentd\n      options:\n        fluentd-address: 127.0.0.1:24224\n        fluentd-async: 'true'\n        tag: docker.cloudflareTraefik\n</code></pre> The Compose file defines a container for Traefik and a container for cloudflared. The Docker network cftunnel-transport is used for transport between Traefik and cloudflared. The Docker network cloudflaretunnel is used to expose Docker containers to Traefik.</p> <p>To secure traffic between Traefik and cloudflared, a Cloudflare Origin Certificate is used. This can be generated in the Cloudflare dashboard and the files should be saved as mydomain.tld.pem and mydomain.tld.key into the origin-certificates folder. We will instruct Traefik to secure all TLS traffic with these certificates. <pre><code>tls:\n  stores:\n    default:\n      defaultCertificate:\n        certFile: /origin-certificates/rorymac.com.pem\n        keyFile: /origin-certificates/rorymac.com.key\n\n  certificates:\n    - certFile: /origin-certificates/rorymac.com.pem\n      keyFile: /origin-certificates/rorymac.com.key\n</code></pre> traefik.yaml file: <pre><code>log:\n  level: DEBUG\n\nentryPoints:\n  websecure:\n    address: \":443\"\n\nproviders:\n  docker:\n    endpoint: \"unix:///var/run/docker.sock\"\n    exposedByDefault: false\n  file:\n    filename: certificates.yaml\n</code></pre></p>"},{"location":"cloudflared/traefik/cloudflared-traefik/#cloudflare-setup","title":"Cloudflare setup","text":"<p>Login to the cloudflare portal and head over to Zero Trust &gt; Networks &gt; Tunnels</p> <p>Add a new tunnel and configure the following:  Add a public host name and configure the following:  </p> <p>To configure a service in the Cloudflare tunnel, add simply https://traefik as the destination. For Traefik to know which service to route the request to, we also have to specify the origin server name. Since Traefik can also speak HTTP/2, we can enable that as well. Also enable No TLS Verify</p>"},{"location":"cloudflared/traefik/cloudflared-traefik/#adding-a-service-to-the-cloudflared-traefik-tunnel","title":"Adding a service to the cloudflared / traefik tunnel","text":"<pre><code>services:\n\n  vaultwarden:\n    container_name: vaultwarden\n    hostname: vaultwarden\n    image: vaultwarden/server:1.30.1\n    restart: unless-stopped\n    networks:\n       cloudflaretunnel:\n    expose:\n      - 80                 # Web UI\n      - 443                # Web UI\n    env_file:\n      - container-vars.env\n    volumes:\n      - /etc/localtime:/etc/localtime:ro\n      - ./data:/data\n    labels:\n      - traefik.enable=true\n      - traefik.http.routers.vaultwarden.rule=Host(`vaultwarden.rorymac.com`)\n      - traefik.http.routers.vaultwarden.entrypoints=websecure\n      - traefik.http.routers.vaultwarden.tls=true\n      - traefik.http.routers.vaultwarden.service=vaultwarden\n      - traefik.http.services.vaultwarden.loadbalancer.server.port=80\nnetworks:\n  cloudflaretunnel:\n    external: true\n</code></pre>"},{"location":"docker/dev_container/","title":"\ud83d\udc33 Homelab Container","text":"<p>This repository provides detailed instructions on building and deploying a homelab dev container.</p>"},{"location":"docker/dev_container/#table-of-contents","title":"\ud83d\udcc4 Table of Contents","text":"<ul> <li>\ud83d\udd28 GitLab CI/CD Build</li> <li>\ud83d\udcda How to deploy the DEV Container</li> </ul>"},{"location":"docker/dev_container/#contact","title":"\ud83d\udceb Contact","text":""},{"location":"docker/dev_container/BUILD/","title":"\ud83d\udd28 Docker-Compose Build","text":"<p>This guide will walk you through the steps to build the dev-container image.</p>"},{"location":"docker/dev_container/BUILD/#build-steps","title":"\ud83d\ude80 Build Steps","text":""},{"location":"docker/dev_container/BUILD/#prerequisites","title":"Prerequisites","text":"<p>Make sure the ca.crt (homelab root certificate) and the python pip requirements.txt file are in the same director as the Dockerfile</p> <ol> <li> <p>Go to your home directory</p> <pre><code>cd $HOME\n</code></pre> </li> <li> <p>git clone https://gitlab-docker01.internal/homelab/containers/dev.git</p> </li> <li> <p>Go to the cloned folder.</p> <pre><code>cd $HOME/dev\n</code></pre> </li> <li> <p>Update the Docker file</p> </li> <li> <p>Update Image and Tag and software version tags in the gitlab-ci.yml file</p> </li> <li> <p>Push code to a feature branch and check the hadolint analyze stage</p> </li> <li> <p>Create a git tag so the build phase begins</p> </li> <li> <p>Wait for the build phase to complete and merge the feature branch to the main branch, this will then create a gitlab release</p> </li> </ol>"},{"location":"docker/dev_container/BUILD/#contact","title":"\ud83d\udceb Contact","text":""},{"location":"docker/dev_container/DEPLOY/","title":"\ud83d\udcda How to deploy the DEV Container","text":"<p>This guide will walk you through how to deploy a docker development container.</p>"},{"location":"docker/dev_container/DEPLOY/#deployment-steps","title":"\ud83d\ude80 Deployment Steps","text":"<ol> <li> <p>Run: docker login registry-gitlab-docker01.internal         docker pull registry-gitlab-docker01.internal/homelab/containers/dev:latest</p> </li> <li> <p>Go to your home directory</p> <pre><code>cd $HOME\n</code></pre> </li> <li> <p>git clone https://gitlab-docker01.internal/homelab/containers/dev.git</p> </li> <li> <p>Go to the cloned folder.</p> <pre><code>cd $HOME/dev\n</code></pre> </li> <li> <p>Copy the sample env.example file:</p> <pre><code>cp -R .env.example .env.user\n</code></pre> </li> <li> <p>Update the following environment variables in the .env.user file</p> </li> <li>IMAGE_NAME</li> <li>IMAGE_TAG</li> <li> <p>USER</p> </li> <li> <p>Copy your host .gitconfig to /home/{TGI}/container_config/</p> <pre><code>cp $HOME/.gitconfig $HOME/container_config/\n</code></pre> </li> <li> <p>Copy your host .bashrc to /home/{TGI}/container_config/</p> <pre><code>cp $HOME/.bashrc $HOME/container_config/\n</code></pre> </li> <li> <p>To deploy your dev container run:</p> <pre><code>docker-compose --env-file .env.user up -d\n</code></pre> </li> <li> <p>To use your dev container run:</p> <pre><code>docker exec -it {TGI}_devcontainer /bin/bash\n</code></pre> </li> <li> <p>Your host working directory structure should look like the following example:</p> <pre><code>.\n\u251c\u2500\u2500 home\n\u2502   \u251c\u2500\u2500 {USER}\n\u2502   \u2502   \u251c\u2500\u2500 container_config\n\u2502   \u2502   \u2502   |\n\u2502   \u2502   \u2502   |\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 .bashrc\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 .gitconfig\n\u2502   \u2502   \u2514\u2500\u2500 dev-container\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2502   \u2514\u2500\u2500 git\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 {PROJECT-A}\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 {PROJECT-B}\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 ...\n</code></pre> </li> <li> <p>To stop your dev container run:</p> <pre><code>docker-compose --env-file .env.user down\n</code></pre> </li> </ol>"},{"location":"docker/dev_container/DEPLOY/#contact","title":"\ud83d\udceb Contact","text":""},{"location":"gitops/fluxcd/components_fluxcd/","title":"FluxCD components","text":""},{"location":"gitops/fluxcd/components_fluxcd/#fluxcd-components","title":"FluxCD Components","text":"<p>FluxCD is a GitOps tool that automates the deployment of applications and infrastructure to Kubernetes. Below is an overview of the main FluxCD components and their roles within the GitOps workflow:</p>"},{"location":"gitops/fluxcd/components_fluxcd/#1-gitrepository","title":"1. GitRepository","text":"<p>The <code>GitRepository</code> resource defines a connection between FluxCD and your Git repository. FluxCD uses this resource to track changes in the repository and automatically apply those changes to your Kubernetes cluster.</p>"},{"location":"gitops/fluxcd/components_fluxcd/#example","title":"Example:","text":"<pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: GitRepository\nmetadata:\n  name: flux-system\n  namespace: flux-system\nspec:\n  interval: 1m\n  url: https://gitlab.com/your-repo/your-repo-name.git\n  branch: main\n  ref:\n    branch: main\n  secretRef:\n    name: git-credentials\n</code></pre> <ul> <li>interval: How often FluxCD checks for updates in the repository.</li> <li>url: The URL of your Git repository.</li> <li>branch: The branch that FluxCD will track (usually main).</li> <li>secretRef: A reference to a Kubernetes secret for Git repository authentication.</li> </ul>"},{"location":"gitops/fluxcd/components_fluxcd/#2-kustomization","title":"2. Kustomization","text":"<p>The Kustomization resource defines how FluxCD should apply the resources in your Git repository to your Kubernetes cluster. It specifies the source, the path to the resources, and the intervals at which to check for updates.</p> <p>Example: <pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: apps\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  dependsOn:\n    - name: ingress-nginx\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./apps/production\n  prune: true\n  wait: true\n  timeout: 5m0s\n</code></pre></p> <ul> <li>interval: How often FluxCD checks for updates in the specified path.</li> <li>dependsOn: Specifies dependencies between different - Kustomizations to ensure correct deployment order.</li> <li>sourceRef: Refers to the GitRepository resource.</li> <li>path: The path in the repository that contains the Kubernetes manifests or Helm charts to be applied.</li> <li>prune: Ensures that resources deleted from the repository are also removed from the cluster.</li> <li>wait: Waits for all resources to become ready before proceeding.</li> <li>timeout: Specifies the maximum time to wait for resources to be ready.</li> </ul>"},{"location":"gitops/fluxcd/components_fluxcd/#3-helmrepository","title":"3. HelmRepository","text":"<p>The HelmRepository resource defines an external Helm chart repository from which FluxCD can pull charts. It allows you to use pre-built Helm charts for deploying applications or infrastructure components.</p> <p>Example:</p> <p><pre><code>apiVersion: source.toolkit.fluxcd.io/v1beta2\nkind: HelmRepository\nmetadata:\n  name: nfs-subdir-external-provisioner\n  namespace: flux-system\nspec:\n  interval: 60m\n  url: https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner\n</code></pre> - interval: Specifies how often to check the repository for new charts. - url: The URL of the Helm repository.</p>"},{"location":"gitops/fluxcd/components_fluxcd/#4-helmrelease","title":"4. HelmRelease","text":"<p>The HelmRelease resource manages the lifecycle of a Helm chart deployment. It uses a HelmRepository resource as the source and applies the Helm chart to your cluster.</p> <p>Example:</p> <pre><code>apiVersion: helm.toolkit.fluxcd.io/v2beta2\nkind: HelmRelease\nmetadata:\n  name: nfs-subdir-external-provisioner\n  namespace: nfs-subdir-external-provisioner\nspec:\n  interval: 30m\n  chart:\n    spec:\n      chart: nfs-subdir-external-provisioner\n      version: \"4.0.18\"\n      sourceRef:\n        kind: HelmRepository\n        name: nfs-subdir-external-provisioner\n        namespace: flux-system\n</code></pre> <ul> <li>interval: How often to check for chart updates.</li> <li>chart: Specifies the Helm chart and version to deploy.</li> <li>sourceRef: References the HelmRepository that contains the chart.</li> </ul>"},{"location":"gitops/fluxcd/components_fluxcd/#5-imagerepository-and-imagepolicy-optional","title":"5. ImageRepository and ImagePolicy (Optional)","text":"<p>If your deployment involves container images, the ImageRepository and ImagePolicy resources manage the discovery and updates of container images.</p> <p>ImageRepository: Monitors a container image repository for new tags. ImagePolicy: Defines rules for selecting which image tag to deploy (e.g., latest stable version).</p> <p>Example:</p> <pre><code>apiVersion: image.toolkit.fluxcd.io/v1alpha1\nkind: ImageRepository\nmetadata:\n  name: my-app\n  namespace: flux-system\nspec:\n  interval: 5m\n  image: my-docker-registry.com/my-app\n</code></pre> <pre><code>apiVersion: image.toolkit.fluxcd.io/v1alpha1\nkind: ImagePolicy\nmetadata:\n  name: my-app-policy\n  namespace: flux-system\nspec:\n  imageRepositoryRef:\n    name: my-app\n  policy:\n    semver:\n      range: 1.0.x\n</code></pre>"},{"location":"gitops/fluxcd/components_fluxcd/#6-notifications-optional","title":"6. Notifications (Optional)","text":"<p>FluxCD can also integrate with external systems like Slack or Microsoft Teams to notify you of changes in the GitOps pipeline. This can be configured using the Alert and Receiver resources.</p> <p>Example:</p> <pre><code>apiVersion: notification.toolkit.fluxcd.io/v1beta1\nkind: Alert\nmetadata:\n  name: flux-alert\n  namespace: flux-system\nspec:\n  providerRef:\n    name: slack\n  eventSeverity: info\n  eventSources:\n    - kind: Kustomization\n      name: apps\n</code></pre>"},{"location":"gitops/fluxcd/my_fluxcd/","title":"My Configuration","text":""},{"location":"gitops/fluxcd/my_fluxcd/#fluxcd-gitops-bootstrap-with-gitlab","title":"FluxCD GitOps Bootstrap with GitLab","text":"<p>This section provides instructions for bootstrapping FluxCD with GitLab using a Personal Access Token (PAT). Follow these steps to create a FluxCD setup that automatically synchronizes your Kubernetes manifests with a GitLab repository.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#steps","title":"Steps","text":""},{"location":"gitops/fluxcd/my_fluxcd/#1-create-the-directory-structure","title":"1. Create the Directory Structure","text":"<p>First, create the necessary directory structure for FluxCD to store the cluster configurations.</p> <p><pre><code>mkdir -p clusters/k8s-prod\n</code></pre> This command creates the clusters/k8s-prod directory where FluxCD will store and apply your Kubernetes manifests.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#2-create-a-personal-access-token-pat-in-gitlab","title":"2. Create a Personal Access Token (PAT) in GitLab","text":"<p>To authenticate FluxCD with GitLab, you need to create a Personal Access Token (PAT) with the api scope in GitLab.</p> <ol> <li>Go to GitLab and navigate to User Settings &gt; Access Tokens.</li> <li>Create a new token with the following settings:<ul> <li>Name: FluxCD Token</li> <li>Scopes: Select the api scope.</li> </ul> </li> <li>Copy the generated token and store it in a safe place. You will use this token to authenticate FluxCD with GitLab.</li> </ol>"},{"location":"gitops/fluxcd/my_fluxcd/#3-set-the-personal-access-token-pat-as-an-environment-variable","title":"3. Set the Personal Access Token (PAT) as an Environment Variable","text":"<p>Export the GitLab PAT token as an environment variable in your terminal.</p> <pre><code>export GITLAB_TOKEN=&lt;token here&gt;\n</code></pre>"},{"location":"gitops/fluxcd/my_fluxcd/#4-bootstrap-fluxcd-with-gitlab","title":"4. Bootstrap FluxCD with GitLab","text":"<p>Now, bootstrap FluxCD with your GitLab repository using the following command:</p> <pre><code>flux bootstrap gitlab --token-auth --ca-file=ca.crt --hostname=https://gitlab-docker01.internal --owner=\"homelab\" --repository \"flux-k8s-gitops\" --branch=main --path=clusters/k8s-prod\n</code></pre>"},{"location":"gitops/fluxcd/my_fluxcd/#explanation-of-flags","title":"Explanation of Flags:","text":"<ul> <li><code>--token-auth</code>: Specifies that the authentication will be done using the GitLab PAT token.</li> <li><code>--ca-file=ca.crt</code>: Specifies the CA file for SSL verification (if using internal GitLab hosting with custom CA).</li> <li><code>--hostname=https://gitlab-docker01.internal</code>: The URL of your self-hosted GitLab instance.</li> <li><code>--owner=\"homelab\"</code>: The GitLab group or user that owns the repository.</li> <li><code>--repository \"flux-k8s-gitops\"</code>: The GitLab repository where FluxCD will store and track the manifests.</li> <li><code>---branch=main</code>: The Git branch to track for changes.</li> <li><code>--path=clusters/k8s-prod</code>: The path within the repository where the cluster configurations are stored.</li> </ul>"},{"location":"gitops/fluxcd/my_fluxcd/#5-verify-the-bootstrap","title":"5. Verify the Bootstrap","text":"<p>After running the bootstrap command, FluxCD should automatically install its components and begin tracking your GitLab repository for changes.</p> <p>To verify that FluxCD has been successfully bootstrapped, use the following command:</p> <p><pre><code>flux check\n</code></pre> This command checks the status of FluxCD and confirms that it has successfully synchronized with your GitLab repository.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#fluxcd-gitops-monorepo-deployment-with-base-and-overlay-structure","title":"FluxCD GitOps (Monorepo) Deployment with Base and Overlay Structure","text":"<p>My repository is structured for deploying applications and infrastructure using FluxCD in a Kubernetes cluster. Everything which is deployed is contained in a single git repository. It follows a base and overlay pattern, with base Helm configurations and environment-specific patches for production. Additionally, the <code>flux-system</code> directory contains the FluxCD bootstrap components, and Flux is configured to automatically apply changes when they are pushed to the main branch.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#repository-structure","title":"Repository Structure","text":"<p>The repository is organized as follows:</p> <ul> <li><code>apps/</code></li> <li><code>base/</code>: Contains the base Helm configurations for all applications.</li> <li> <p><code>production/</code>: Contains overlay configurations to patch the base for production deployments.</p> </li> <li> <p><code>infrastructure/</code></p> </li> <li><code>base/</code>: Contains the base configurations for infrastructure components (e.g., Grafana, NGINX Ingress, Loki, MetalLB, NFS Subdir External Provisioner etc).</li> <li> <p><code>production/</code>: Contains overlay configurations for infrastructure components for the production environment.</p> </li> <li> <p><code>clusters/</code></p> </li> <li><code>k8s-prod/</code><ul> <li><code>infrastructure.yaml</code>: Contains the            kustomization files to deploy the infrastructure components.</li> <li><code>apps.yaml</code>: Contains the kustomization files to recursively deploy applications.</li> <li><code>flux-system/</code>: Contains the FluxCD bootstrap components, including the controllers and initial configuration for GitOps integration.</li> </ul> </li> </ul>"},{"location":"gitops/fluxcd/my_fluxcd/#key-components","title":"Key Components","text":""},{"location":"gitops/fluxcd/my_fluxcd/#base-configurations-appsbase","title":"Base Configurations (<code>apps/base</code>)","text":"<p>The <code>apps/base</code> directory contains the default Helm configurations for the applications. These are reusable across different environments (e.g., production, staging, development).</p>"},{"location":"gitops/fluxcd/my_fluxcd/#production-overlays-appsproduction","title":"Production Overlays (<code>apps/production</code>)","text":"<p>The <code>apps/production</code> directory contains the overlays to patch the base configurations specifically for the production environment. These overlays may modify the base configurations to adjust resource limits, replicas, or other production-specific settings.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#cluster-specific-kustomization-clustersk8s-prod","title":"Cluster-Specific Kustomization (<code>clusters/k8s-prod/</code>)","text":"<p>The <code>clusters/k8s-prod</code> directory contains the <code>Kustomization</code> resource to apply the production environment configuration to the cluster.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#flux-bootstrap-components-clustersflux-system","title":"Flux Bootstrap Components (<code>clusters/flux-system/</code>)","text":"<p>The <code>clusters/flux-system</code> directory contains the FluxCD bootstrap components, which manage the GitOps workflow, monitor the repository, and apply changes automatically when updates are pushed to the main branch.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#example-kustomizationyaml-for-production","title":"Example <code>Kustomization.yaml</code> for Production","text":"<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: apps\n  namespace: flux-system\nspec:\n  interval: 10m0s\n  dependsOn:\n    - name: ingress-nginx\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./apps/production\n  prune: true\n  wait: true\n  timeout: 5m0s\n</code></pre>"},{"location":"gitops/fluxcd/my_fluxcd/#key-parameters","title":"Key Parameters","text":"<ul> <li>interval: FluxCD will check for updates every 10 minutes.</li> <li>dependsOn: Specifies that the deployment of the applications depends on the successful deployment of the ingress-nginx Kustomization.</li> <li>sourceRef: Points to the GitRepository resource (flux-system), which provides the configurations for deployment.</li> <li>path: Specifies the path to the production-specific overlay configurations (./apps/production).</li> <li>prune: Ensures that resources no longer present in the Git repository are deleted from the cluster.</li> <li>wait: Ensures that FluxCD waits for all resources to be fully ready before proceeding.</li> <li>timeout: Sets the maximum wait time for resources to become ready (5 minutes).</li> <li>Deployment Process</li> </ul>"},{"location":"gitops/fluxcd/my_fluxcd/#infrastructure-kustomizations","title":"Infrastructure Kustomizations","text":"<p>The <code>infrastructure.yaml</code> file in the <code>clusters/k8s-prod/</code> directory includes multiple Kustomizations for various infrastructure components. These Kustomizations specify dependencies, intervals, retry intervals, timeouts, and paths for each component.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#example-kustomizations-in-infrastructureyaml","title":"Example Kustomizations in <code>infrastructure.yaml</code>","text":"<pre><code>apiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: ingress-nginx\n  namespace: flux-system\nspec:\n  dependsOn:\n  - name: metallb\n  interval: 1h\n  retryInterval: 1m\n  timeout: 15m\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./infrastructure/production/ingress-nginx\n  prune: true\n  wait: true\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: cert-manager\n  namespace: flux-system\nspec:\n  interval: 1h\n  retryInterval: 1m\n  timeout: 5m\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./infrastructure/production/cert-manager\n  prune: true\n  wait: true\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: infra-configs\n  namespace: flux-system\nspec:\n  dependsOn: \n  - name: ingress-nginx\n  - name: cert-manager\n  interval: 1h\n  retryInterval: 1m\n  timeout: 5m\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./infrastructure/production/configs\n  prune: true\n  wait: true\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: metallb\n  namespace: flux-system\nspec:\n  interval: 1h\n  retryInterval: 1m\n  timeout: 5m\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./infrastructure/production/metallb\n  prune: true\n  wait: true\n---\napiVersion: kustomize.toolkit.fluxcd.io/v1\nkind: Kustomization\nmetadata:\n  name: grafana\n  namespace: flux-system\nspec:\n  interval: 5m\n  dependsOn:\n  - name: ingress-nginx\n  - name: nfs-subdir-external-provisioner\n  sourceRef:\n    kind: GitRepository\n    name: flux-system\n  path: ./infrastructure/production/grafana\n  prune: true\n  wait: true\n</code></pre> <p>Since FluxCD is bootstrapped to monitor the main branch of this repository for changes, any updates pushed to the repository will be automatically applied to the cluster. There\u2019s no need to manually run kubectl apply -k.</p>"},{"location":"gitops/fluxcd/my_fluxcd/#how-it-works","title":"How It Works","text":"<p>Push Changes: When you push new changes or updates to the main branch, FluxCD will automatically detect the changes and apply them to your Kubernetes cluster. Monitor the Deployment: </p> <p>You can monitor the status of the FluxCD deployment using the following commands:</p> <pre><code>flux get kustomizations -n flux-system # Gets all the kustomizations in that namespace\n\nflux get helmrelease ingress-nginx -n ingress-nginx # Gets helm releases in that namespace\n\nflux events -n ingress-nginx # This gets all flux events in that namespace\n</code></pre>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/","title":"How to structure your repository","text":"<p>When structuring a FluxCD repository, the goal is to organize your Kubernetes and infrastructure manifests in a way that supports GitOps principles: having Git as the single source of truth, enabling automated, repeatable, and declarative deployments.</p> <p>Here are some common ways of structuring a FluxCD repository:</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#1-mono-repository-monorepo","title":"1. Mono Repository (Monorepo)","text":"<p>In a mono repository, both applications and infrastructure components are stored in a single Git repository. This structure centralizes all the configurations for managing your cluster.</p> <p>Example Structure:</p> <pre><code>\u251c\u2500\u2500 apps/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 production/\n\u251c\u2500\u2500 clusters/\n\u2502   \u251c\u2500\u2500 k8s-prod/\n\u2502   \u2502   \u251c\u2500\u2500 infrastructure.yaml\n\u2502   \u2502   \u251c\u2500\u2500 apps.yaml\n\u2502   \u2514\u2500\u2500 flux-system/\n\u251c\u2500\u2500 infrastructure/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 production/\n\u2514\u2500\u2500 .flux.yaml\n</code></pre> <ul> <li><code>apps/</code>: Stores your application configurations.</li> <li>base/: Reusable base configurations for all environments.</li> <li>production/: Environment-specific overlays for production.</li> <li><code>clusters/k8s-prod/</code>: Contains Kustomizations that reference the GitRepository and paths for both apps/ and infrastructure/.</li> <li><code>infrastructure/</code>: Contains infrastructure components (e.g., NGINX Ingress, Grafana).</li> <li>base/: Common base configuration for all infrastructure components.</li> <li>production/: Production-specific overlays to adjust resources and other settings.</li> </ul> <p>Advantages:</p> <p>Centralized management of both apps and infrastructure. Simplifies versioning, as everything is managed in one repository.</p> <p>Disadvantages:</p> <p>Can grow large and complex as more components are added. Can create longer commit histories that are harder to manage.</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#2-multi-repository-polyrepo","title":"2. Multi-Repository (Polyrepo)","text":"<p>In a multi-repository structure, different Git repositories are used for various components, such as applications, infrastructure, and cluster configurations. Each repository handles a specific aspect of the cluster, and FluxCD pulls from each.</p> <p>Example:</p> <p><pre><code>app-repo/\n\u2514\u2500\u2500 apps/\n    \u251c\u2500\u2500 base/\n    \u2514\u2500\u2500 production/\n\ninfra-repo/\n\u2514\u2500\u2500 infrastructure/\n    \u251c\u2500\u2500 base/\n    \u2514\u2500\u2500 production/\n\nclusters-repo/\n\u2514\u2500\u2500 clusters/\n    \u251c\u2500\u2500 k8s-prod/\n    \u2514\u2500\u2500 flux-system/\n</code></pre> - <code>app-repo/</code>: Contains only application configurations. - <code>infra-repo/</code>: Contains infrastructure components. - <code>clusters-repo/</code>: Contains cluster configurations and FluxCD bootstrap components.</p> <p>Advantages:</p> <p>Separation of concerns: app, infra, and cluster management are handled independently. Teams can work on their own repositories without affecting others.</p> <p>Disadvantages:</p> <p>Complex management: requires handling multiple repositories, coordinating changes, and ensuring compatibility. Need for automation tools to handle cross-repo synchronization.</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#3-environment-based-structure","title":"3. Environment-Based Structure","text":"<p>Each environment (e.g., dev, staging, prod) has its own directory or repository. This structure helps manage separate environments with isolated configurations.</p> <p>Example:</p> <p><pre><code>flux-repo/\n\u251c\u2500\u2500 dev/\n\u2502   \u251c\u2500\u2500 apps/\n\u2502   \u2514\u2500\u2500 infrastructure/\n\u251c\u2500\u2500 staging/\n\u2502   \u251c\u2500\u2500 apps/\n\u2502   \u2514\u2500\u2500 infrastructure/\n\u2514\u2500\u2500 prod/\n    \u251c\u2500\u2500 apps/\n    \u2514\u2500\u2500 infrastructure/\n</code></pre> - <code>dev/</code>, <code>staging/</code>, <code>prod/</code>: Each environment is separated with its own applications and infrastructure configurations.</p> <p>Advantages:</p> <p>Isolates environments completely, preventing accidental deployment of configurations across environments. Clear separation of deployment environments and their respective states.</p> <p>Disadvantages:</p> <p>Potential duplication of configurations if the same applications or infrastructure components are used across environments. More work to maintain consistency across environments.</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#4-component-based-structure","title":"4. Component-Based Structure","text":"<p>This structure organizes the repository based on individual components like ingress, monitoring, storage, etc., with each component having its own folder.</p> <p>Example:</p> <p><pre><code>Copy code\nflux-repo/\n\u251c\u2500\u2500 ingress/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 overlays/\n\u251c\u2500\u2500 monitoring/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 overlays/\n\u251c\u2500\u2500 storage/\n\u2502   \u251c\u2500\u2500 base/\n\u2502   \u2514\u2500\u2500 overlays/\n\u2514\u2500\u2500 .flux.yaml\n</code></pre> - <code>ingress/</code>: Contains configurations for NGINX Ingress, cert-manager, etc. - <code>monitoring/</code>: Contains configurations for Grafana, Prometheus, Loki, etc. - <code>storage/</code>: Contains configurations for Longhorn, NFS Subdir External Provisioner, etc.</p> <p>Advantages:</p> <p>Allows modular management of different components. Each component can evolve independently with its own base and overlay structures.</p> <p>Disadvantages:</p> <p>Requires careful management of dependencies between components. Can become complex if components interact heavily.</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#5-tenant-based-structure-multi-tenancy","title":"5. Tenant-Based Structure (Multi-Tenancy)","text":"<p>This structure is used for managing multiple tenants within a single cluster. Each tenant\u2019s applications and configurations are isolated in their own directory or repository.</p> <p>Example:</p> <p><pre><code>flux-repo/\n\u251c\u2500\u2500 tenants/\n\u2502   \u251c\u2500\u2500 tenant-a/\n\u2502   \u2502   \u251c\u2500\u2500 apps/\n\u2502   \u2502   \u2514\u2500\u2500 infrastructure/\n\u2502   \u2514\u2500\u2500 tenant-b/\n\u2502       \u251c\u2500\u2500 apps/\n\u2502       \u2514\u2500\u2500 infrastructure/\n\u2514\u2500\u2500 clusters/\n    \u2514\u2500\u2500 k8s-prod/\n</code></pre> - <code>tenants/</code>: Each tenant has its own isolated directory with applications and infrastructure configurations. - <code>clusters/</code>: The cluster-level configurations that apply to the overall cluster.</p> <p>Advantages:</p> <p>Provides strong isolation between tenants. Supports multi-tenancy use cases where different teams or customers need separate environments.</p> <p>Disadvantages:</p> <p>Can lead to duplication of configuration files across tenants. More work required to maintain consistency across tenants.</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#6-single-source-of-truth-environment-branches","title":"6. Single Source of Truth (Environment Branches)","text":"<p>In this structure, each environment (dev, staging, prod) is represented by different Git branches. FluxCD monitors these branches and applies the configurations accordingly.</p> <p>Example:</p> <ul> <li><code>main</code>: Represents production.</li> <li><code>staging</code>: Represents the staging environment.</li> <li><code>dev</code>: Represents the development environment.</li> </ul> <p>FluxCD is configured to monitor the correct branch based on the environment.</p> <p>Advantages:</p> <p>Simple and easy to understand branching strategy. Easy rollback by switching branches.</p> <p>Disadvantages:</p> <p>Can cause branching conflicts if not carefully managed. Merge conflicts might arise when promoting changes from dev to staging or production.</p>"},{"location":"gitops/fluxcd/structuring_repo_fluxcd/#choosing-the-right-structure","title":"Choosing the Right Structure","text":"<ul> <li>Monorepo is suitable for smaller teams or projects that benefit from centralized management.</li> <li>Polyrepo is better for larger organizations with separate teams managing different parts of the system.</li> <li>Environment-based structures help when environments need to be strongly isolated from each other.</li> <li>Component-based structures are ideal when managing a complex system where components evolve independently.</li> <li>Tenant-based structures are good for multi-tenant systems where each tenant requires isolated configurations.</li> </ul>"},{"location":"kubernetes/kubespray/upgrade-process/","title":"Upgrade process","text":"<ol> <li>git switch -c release-2.26</li> <li>git fetch upstream release-2.26</li> <li>git pull upstream release-2.26 --force</li> <li>git merge upstream/release-2.26</li> <li>change k8s version in vars and run:    git commit -m \"updating cluster to kubespray release 2.26\"</li> </ol>"},{"location":"linux_admin/change_dns_config_cloud-init_vm/","title":"How to change DNS servers on Ubuntu server","text":"<ol> <li>sudo -i</li> <li>resolvectl status</li> <li>sudo apt install openvswitch-switch-dpdk</li> <li>sudo bash -c 'echo \"network: {config: disabled}\" &gt; /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</li> <li>sudo chmod 600 /etc/netplan/50-cloud-init.yaml  </li> <li>nano /etc/netplan/50-cloud-init.yaml  ## add name servers  </li> <li>sudo netplan apply</li> <li>systemctl restart kubelet</li> <li>sudo reboot -f</li> </ol>"},{"location":"proxmox/delete_existing_vm_conf/","title":"Delete vm conf file","text":"<ol> <li>ssh to pve node</li> <li>cd etc/pve/qemu-server/</li> <li>rm -rf VMID.conf </li> </ol>"},{"location":"terraform/proxmox_module/","title":"Proxmox VM Provisioning with Terraform","text":"<p>This Terraform configuration automates the provisioning and configuration of virtual machines (VMs) on a Proxmox Virtual Environment (PVE) server. By defining VM specifications in a structured format, you can deploy multiple VMs with cloud-init configurations, custom resource allocations, and QEMU guest agent support.</p>"},{"location":"terraform/proxmox_module/#overview","title":"Overview","text":"<p>This Terraform setup allows you to create and configure VMs on a Proxmox server by specifying key attributes like CPU, memory, disk storage, network settings, and cloud-init customization. The configuration also supports cloning existing VM templates and configuring VMs with dynamic disk attachments.</p>"},{"location":"terraform/proxmox_module/#key-features","title":"Key Features","text":"<ul> <li>Customizable VM Specs: Configure CPU cores, memory, storage, and network settings.</li> <li>Cloud-Init Support: Customize initialization scripts, SSH keys, and packages via cloud-init.</li> <li>Agent Integration: Optionally enable QEMU guest agent for enhanced VM functionality.</li> <li>Dynamic Disks: Attach multiple storage disks dynamically.</li> <li>Detailed Outputs: Get essential details of the provisioned VMs for further automation.</li> </ul>"},{"location":"terraform/proxmox_module/#requirements","title":"Requirements","text":"Name Version terraform &gt;= 0.14 proxmox 0.43.2"},{"location":"terraform/proxmox_module/#providers","title":"Providers","text":"Name Version proxmox 0.43.2"},{"location":"terraform/proxmox_module/#modules","title":"Modules","text":"<p>No modules.</p>"},{"location":"terraform/proxmox_module/#resources","title":"Resources","text":"Name Type proxmox_virtual_environment_file.cloud_init resource proxmox_virtual_environment_vm.virtual_machine resource"},{"location":"terraform/proxmox_module/#inputs","title":"Inputs","text":"Name Description Type vm_os The vm operating system name <code>string</code> gateway_ip The network gateway IP <code>string</code> pve_vms n/a <pre>map(object({    name                   = string    clone_id               = number    ipv4_address           = string    memory                 = number    cores                  = number    dns                    = list(string)    sockets                = string    vm_id                  = number    node_name              = string    numa                   = bool    agent_enabled          = bool    agent_timeout          = string    machine                = string    bios                   = string    # startup_order          = number    # startup_delay          = number    # startup_down_delay     = number    os_type                = string    scsi_hardware          = string    vga_memory             = number    vga_enabled            = bool    vga_type               = string    network_device_bridge  = string    vm_description         = string    boot_disk_datastore_id = string    boot_disk_interface    = string    boot_disk_size         = number    disks = list(object({      disk_datastore_id = string      disk_file_format  = string      disk_size         = number      disk_interface    = string    }))    tags = list(string)    # Cloud Init    cloud_init_content_type = string    cloud_init_datastore_id = string    cloud_init_node_name    = string    groups                  = list(string)    sudo_config             = string    packages                = list(string)    username                = string    fullname                = string    hostname                = string    fqdn                    = string    # # Cloud Image    # cloud_image_url          = string    # cloud_image_node_name    = string    # cloud_image_datastore_id = string    # cloud_image_content_type = string              }))</pre> For a complete list of inputs and their descriptions for the Proxmox provider, refer to the Proxmox Provider Documentation."},{"location":"terraform/proxmox_module/#outputs","title":"Outputs","text":"Name Description cloud_init_file_names The cloud init file names ipv4_addresses The IPv4 addresses per network interface published by the QEMU agent (empty list when agent.enabled is false) vm_id The id of the virtual machine vm_names The name of each virtual machine"},{"location":"terraform/proxmox_module/#usage","title":"Usage","text":""},{"location":"terraform/proxmox_module/#providertf","title":"provider.tf","text":"<pre><code>terraform {\n  required_version = \"&gt;= 0.14\"\n  required_providers {\n    proxmox = {\n      source  = \"bpg/proxmox\"\n      version = \"0.43.2\"\n    }\n  }\n}\n</code></pre>"},{"location":"terraform/proxmox_module/#cloud-inittf","title":"cloud-init.tf","text":"<pre><code>#===============================================================================\n# Cloud Config (cloud-init)\n#===============================================================================\n\nresource \"proxmox_virtual_environment_file\" \"cloud_init\" {\n  for_each = var.pve_vms\n\n  content_type = each.value.cloud_init_content_type\n  datastore_id = each.value.cloud_init_datastore_id\n  node_name    = each.value.cloud_init_node_name\n\n  source_raw {\n    data = &lt;&lt;EOF\n#cloud-config\nhostname: ${each.value.hostname}\nfqdn: ${each.value.hostname}\nmanage_etc_hosts: true\nlocale: en_GB.UTF-8\ntimezone: Europe/London\nchpasswd:\n  ## Forcing user to change the default password at first login\n  expire: true\nusers:\n  - name: ${each.value.username}\n    gecos: ${each.value.fullname}\n    sudo: \"${join(\", \", each.value.sudo_config)}\"\n    groups: \"${join(\", \", each.value.groups)}\"\n    lock_passwd: false\n    ssh_authorized_keys:\n    %{ for key in each.value.ssh_authorized_keys ~}\n      - ${key}\n    %{ endfor ~}\nchpasswd:\n  ## Forcing user to change the default password at first login\n  expire: true\n  list: |\n    rory:password\npackage_update: true\npackage_upgrade: true\n${length(each.value.packages) &gt; 0 ? \"packages:\\n${join(\"\\n\", [for pkg in each.value.packages : \"  - ${pkg}\"])}\" : \"\"}\nruncmd:\n  - systemctl enable qemu-guest-agent\n  - systemctl start qemu-guest-agent\npower_state:\n    delay: now\n    mode: reboot\n    message: Rebooting after cloud-init completion\n    condition: true\n\n  EOF\n\n    file_name = \"pve-vm-${each.value.name}-cloud-init.yaml\"\n  }\n}\n</code></pre>"},{"location":"terraform/proxmox_module/#vmtf","title":"vm.tf","text":"<pre><code>resource \"proxmox_virtual_environment_vm\" \"virtual_machine\" {\n  for_each = var.pve_vms\n\n  name        = each.value.name\n  description = each.value.vm_description\n  node_name   = each.value.node_name\n  migrate     = false // migrate the VM on node change\n  vm_id       = each.value.vm_id\n  tags        = each.value.tags\n\n  clone {\n    vm_id = each.value.clone_id\n  }\n\n  bios    = each.value.bios\n  machine = each.value.machine\n\n  memory {\n    dedicated = each.value.memory\n  }\n\n  cpu {\n    cores   = each.value.cores\n    numa    = each.value.numa\n    sockets = each.value.sockets\n\n  }\n\n  serial_device {}\n\n  vga {\n    enabled = each.value.vga_enabled\n    memory  = each.value.vga_memory\n    type    = each.value.vga_type\n\n\n  }\n\n  agent {\n    enabled = each.value.agent_enabled\n    timeout = each.value.agent_timeout\n  }\n\n  # startup {\n  #   order      = each.value.startup_order\n  #   up_delay   = each.value.startup_delay\n  #   down_delay = each.value.startup_down_delay\n  # }\n\n  operating_system {\n    type = each.value.os_type\n  }\n\n  scsi_hardware = each.value.scsi_hardware\n\n\n  initialization {\n    ip_config {\n      ipv4 {\n        address = each.value.ipv4_address\n        gateway = var.gateway_ip\n      }\n      ipv6 {\n        address = \"dhcp\"\n\n      }\n    }\n\n    dns {\n      servers = each.value.dns\n    }\n\n    user_data_file_id = proxmox_virtual_environment_file.cloud_init[each.key].id\n  }\n\n  # boot disk\n  disk {\n    datastore_id = each.value.boot_disk_datastore_id\n    interface    = each.value.boot_disk_interface\n    size         = each.value.boot_disk_size\n  }\n\n  # attached disks from data_vm\n  dynamic \"disk\" {\n    for_each = each.value.disks\n    content {\n      datastore_id = disk.value.disk_datastore_id\n      file_format  = disk.value.disk_file_format\n      size         = disk.value.disk_size\n      # assign from scsi1 and up\n      interface = disk.value.disk_interface\n\n\n    }\n\n  }\n}\n</code></pre>"},{"location":"terraform/proxmox_module/#localtf","title":"local.tf","text":"<pre><code>resource \"proxmox_virtual_environment_vm\" \"virtual_machine\" {\n  for_each = var.pve_vms\n\n  name        = each.value.name\n  description = each.value.vm_description\n  node_name   = each.value.node_name\n  migrate     = false // migrate the VM on node change\n  vm_id       = each.value.vm_id\n  tags        = each.value.tags\n\n  clone {\n    vm_id = each.value.clone_id\n }\n\n  bios    = each.value.bios\n  machine = each.value.machine\n\n  memory {\n    dedicated = each.value.memory\n  }\n\n  cpu {\n    cores   = each.value.cores\n    numa    = each.value.numa\n    sockets = each.value.sockets\n\n  }\n\n  serial_device {}\n\n  vga {\n    enabled = each.value.vga_enabled\n    memory  = each.value.vga_memory\n    type    = each.value.vga_type\n\n\n  }\n\n  agent {\n    enabled = each.value.agent_enabled\n    timeout = each.value.agent_timeout\n  }\n\n  # startup {\n  #   order      = each.value.startup_order\n  #   up_delay   = each.value.startup_delay\n  #   down_delay = each.value.startup_down_delay\n  # }\n\n  operating_system {\n    type = each.value.os_type\n  }\n\n  scsi_hardware = each.value.scsi_hardware\n\n\n  initialization {\n    ip_config {\n      ipv4 {\n        address = each.value.ipv4_address\n        gateway = var.gateway_ip\n        }\n      ipv6 {\n        address = \"dhcp\"\n\n      }\n    }\n\n    dns {\n      servers = each.value.dns\n    }\n\n    user_data_file_id = proxmox_virtual_environment_file.cloud_init[each.key].id\n  }\n\n  # boot disk\n  disk {\n    datastore_id = each.value.boot_disk_datastore_id\n    interface    = each.value.boot_disk_interface\n    size         = each.value.boot_disk_size\n  }\n\n  # attached disks from data_vm\n  dynamic \"disk\" {\n    for_each = each.value.disks\n    content {\n      datastore_id = disk.value.disk_datastore_id\n      file_format  = disk.value.disk_file_format\n      size         = disk.value.disk_size\n      # assign from scsi1 and up\n      interface    = disk.value.disk_interface\n\n\n    }\n\n  }\n}\n</code></pre>"},{"location":"terraform/proxmox_module/#variablestf","title":"variables.tf","text":"<pre><code># VM clone\nvariable \"gateway_ip\" {\n  type        = string\n  description = \"The network gateway IP\"\n  default     = \"192.168.7.1\"\n}\n\nvariable \"vm_os\" {\n  type        = string\n  description = \"The vm operating system name\"\n\n}\n\nvariable \"pve_vms\" {\n  type = map(object({\n    name                   = string\n    clone_id               = number\n    ipv4_address           = string\n    memory                 = number\n    cores                  = number\n    dns                    = list(string)\n    sockets                = string\n    vm_id                  = number\n    node_name              = string\n    numa                   = bool\n    agent_enabled          = bool\n    agent_timeout          = string\n    machine                = string\n    bios                   = string\n    # startup_order          = number\n    # startup_delay          = number\n    # startup_down_delay     = number\n    os_type                = string\n    scsi_hardware          = string\n    vga_memory             = number\n    vga_enabled            = bool\n    vga_type               = string\n    network_device_bridge  = string\n    vm_description         = string\n    boot_disk_datastore_id = string\n    boot_disk_interface    = string\n    boot_disk_size         = number\n    disks = list(object({\n      disk_datastore_id = string\n      disk_file_format  = string\n      disk_size         = number\n      disk_interface    = string\n    }))\n    tags = list(string)\n    # Cloud Init\n    cloud_init_content_type = string\n    cloud_init_datastore_id = string\n    cloud_init_node_name    = string\n    groups                  = list(string)\n    sudo_config             = string\n    packages                = list(string)\n    username                = string\n    fullname                = string\n    hostname                = string\n    fqdn                    = string\n    # # Cloud Image\n    # cloud_image_url          = string\n    # cloud_image_node_name    = string\n    # cloud_image_datastore_id = string\n    # cloud_image_content_type = string\n\n\n\n\n  }))\n  default = {}\n}\n</code></pre>"},{"location":"terraform/proxmox_module/#outputstf","title":"outputs.tf","text":"<pre><code>output \"vm_names\" {\n  value       = [for vm_name, vm_config in var.pve_vms : vm_config.name]\n  description = \"The name of each virtual machine\"\n}\n\noutput \"vm_id\" {\n  value = {\n    for vm_name, vm_config in var.pve_vms :\n    vm_name =&gt; proxmox_virtual_environment_vm.virtual_machine[vm_name].id\n  }\n  description = \"The id of the virtual machine\"\n}\n\noutput \"cloud_init_file_names\" {\n  value       = [for key, cloud_init_resource in proxmox_virtual_environment_file.cloud_init : cloud_init_resource.file_name]\n  description = \"The cloud init file names\"\n}\n\n\noutput \"ipv4_addresses\" {\n  value = {\n    for vm_name, vm_config in var.pve_vms :\n    vm_name =&gt; proxmox_virtual_environment_vm.virtual_machine[vm_name].ipv4_addresses\n  }\n  description = \"The IPv4 addresses per network interface published by the QEMU agent (empty list when agent.enabled is false)\"\n}\n</code></pre>"},{"location":"terraform/proxmox_module/#license","title":"License","text":"<p>This project is licensed under the MIT License. See the LICENSE file for details.</p>"},{"location":"terraform/push_module_to_gitlab/","title":"Proxmox Terraform Custom Module","text":""},{"location":"terraform/push_module_to_gitlab/#push-module","title":"push module","text":"<ol> <li>git add .</li> <li>git commit -m \"\"</li> <li>git push</li> <li>git tag -a 1.0.4 -m \"added kubespray ssh key\" </li> <li>git push origin 1.0.5</li> </ol>"}]}